{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fully Convolutional Network","metadata":{}},{"cell_type":"code","source":"# Author : joono\n# Date : 2022-02-10","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:02:08.705762Z","iopub.execute_input":"2022-02-10T14:02:08.706326Z","iopub.status.idle":"2022-02-10T14:02:08.709802Z","shell.execute_reply.started":"2022-02-10T14:02:08.706287Z","shell.execute_reply":"2022-02-10T14:02:08.709021Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport cv2\nfrom tqdm import tqdm\nimport datetime\n\nfrom  matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nfrom IPython.display import clear_output\n%matplotlib inline\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\nimport torch\nimport torch.nn as nn\nimport torchvision \nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport regex","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:04.007167Z","iopub.execute_input":"2022-02-10T13:20:04.007433Z","iopub.status.idle":"2022-02-10T13:20:09.699727Z","shell.execute_reply.started":"2022-02-10T13:20:04.007405Z","shell.execute_reply":"2022-02-10T13:20:09.698958Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"markdown","source":"### Source Dataset","metadata":{}},{"cell_type":"code","source":"# Load directories\ntrain_data_dir = \"../input/kittiroadsegmentation/training/image_2/\"\ntrain_gt_dir = \"../input/kittiroadsegmentation/training/gt_image_2/\"\n\ntest_data_dir = \"../input/kittiroadsegmentation/testing/\"","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:09.701548Z","iopub.execute_input":"2022-02-10T13:20:09.701827Z","iopub.status.idle":"2022-02-10T13:20:09.706075Z","shell.execute_reply.started":"2022-02-10T13:20:09.701791Z","shell.execute_reply":"2022-02-10T13:20:09.705384Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Number of training examples\nTRAINSET_SIZE = int(len(os.listdir(train_data_dir)) * 0.8)\nprint(f\"Number of Training Examples: {TRAINSET_SIZE}\")\n\nVALIDSET_SIZE = int(len(os.listdir(train_data_dir)) * 0.1)\nprint(f\"Number of Validation Examples: {VALIDSET_SIZE}\")\n\nTESTSET_SIZE = int(len(os.listdir(train_data_dir)) - TRAINSET_SIZE - VALIDSET_SIZE)\nprint(f\"Number of Testing Examples: {TESTSET_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:09.707337Z","iopub.execute_input":"2022-02-10T13:20:09.707796Z","iopub.status.idle":"2022-02-10T13:20:09.795455Z","shell.execute_reply.started":"2022-02-10T13:20:09.707761Z","shell.execute_reply":"2022-02-10T13:20:09.794756Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Initialize Constants\nIMG_SIZE = 224\nN_CHANNELS = 3\nN_CLASSES = 1\nSEED = 22022","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:09.797572Z","iopub.execute_input":"2022-02-10T13:20:09.798053Z","iopub.status.idle":"2022-02-10T13:20:09.802036Z","shell.execute_reply.started":"2022-02-10T13:20:09.798018Z","shell.execute_reply":"2022-02-10T13:20:09.801318Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Function to load image and return a dictionary\ndef parse_image(img_path: str) -> dict:\n    image = Image.open(img_path)\n    \n    # Three types of img paths: um, umm, uu\n    # gt image paths: um_road, umm_road, uu_road\n    mask_path = img_path.replace(\"image_2\", \"gt_image_2\")\n    mask_path = mask_path.replace(\"um_\", \"um_road_\")\n    mask_path = mask_path.replace(\"umm_\", \"umm_road_\")\n    mask_path = mask_path.replace(\"uu_\", \"uu_road_\")\n    \n    mask = Image.open(mask_path)\n    \n    return {'image': image, 'mask': mask}","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:09.804141Z","iopub.execute_input":"2022-02-10T13:20:09.804686Z","iopub.status.idle":"2022-02-10T13:20:09.811204Z","shell.execute_reply.started":"2022-02-10T13:20:09.804647Z","shell.execute_reply":"2022-02-10T13:20:09.810450Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"randimg = random.choice(os.listdir(train_data_dir))\n\ndata = parse_image(os.path.join(train_data_dir, randimg))\n\nimg = data[\"image\"]\ngt = data[\"mask\"]\n\nprint(img.size, gt.size)\n\nplt.figure(figsize=(20, 10))\nplt.subplot(121)\nplt.imshow(img)\nplt.subplot(122)\nplt.imshow(gt)\n\ngt = np.array(gt)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:56:36.522346Z","iopub.execute_input":"2022-02-10T13:56:36.522902Z","iopub.status.idle":"2022-02-10T13:56:37.027320Z","shell.execute_reply.started":"2022-02-10T13:56:36.522864Z","shell.execute_reply":"2022-02-10T13:56:37.026583Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class KITTYRoadSegDataset(Dataset):\n    \"\"\"Auth: joono, pytorch version of tf dataset builing\"\"\"\n    \n    def __init__(self, data_dir, transform=None):\n        super().__init__()\n        self.imgs = [os.path.join(data_dir, img) for img in os.listdir(data_dir)] \n        self.transforms = transform\n        \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = self.imgs[idx]\n        data = parse_image(img_name)\n\n        sample = dict()\n        if self.transforms:\n            sample[\"image\"] = self.transforms(data[\"image\"])\n            \n            mask = self.transforms(data[\"mask\"])\n            sample[\"mask\"] = mask[2]\n\n        return sample\n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:10.352817Z","iopub.execute_input":"2022-02-10T13:20:10.353036Z","iopub.status.idle":"2022-02-10T13:20:10.362429Z","shell.execute_reply.started":"2022-02-10T13:20:10.353007Z","shell.execute_reply":"2022-02-10T13:20:10.361707Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trainset = KITTYRoadSegDataset(\n    data_dir=train_data_dir,\n    transform=transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor()\n    ])\n)\n\ntrainloader = DataLoader(\n    dataset=trainset,\n    batch_size=8,\n    shuffle=True,\n    num_workers=2,\n    drop_last=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:10.363851Z","iopub.execute_input":"2022-02-10T13:20:10.364376Z","iopub.status.idle":"2022-02-10T13:20:10.376286Z","shell.execute_reply.started":"2022-02-10T13:20:10.364337Z","shell.execute_reply":"2022-02-10T13:20:10.375579Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Define Network","metadata":{}},{"cell_type":"code","source":"class VGG16_FCN8(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        self.n_classes = n_classes\n        \n        self.W = IMG_SIZE\n        self.H = IMG_SIZE\n        \n        self.vgg = nn.Sequential(*list(torchvision.models.vgg16(pretrained=True).children()))\n        self.pool3 = pool3 = self.vgg[0][:17]\n        self.pool4 = pool4 = self.vgg[0][17:24]\n        self.pool5 = pool5 = self.vgg[0][24:]\n        \n        # Wout, Hout = Win * scale_factor, Hin * scale_factor\n        self.upsampling2 = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.upsampling8 = nn.UpsamplingBilinear2d(scale_factor=8)\n        \n        self.conv = nn.Conv2d(512, 256, kernel_size=(1, 1))\n        \n        self.fc_layer = nn.Sequential(\n            nn.Conv2d(256, self.n_classes, kernel_size=(1, 1)),\n            nn.BatchNorm2d(self.n_classes),\n            nn.ReLU(inplace=True),\n        )\n        \n        \n    def forward(self, x):\n        p3 = self.pool3(x)\n        p4 = self.pool4(p3)\n        p5 = self.pool5(p4)\n        \n        u1 = self.upsampling2(p5)\n        d1 = torch.add(p4, u1)\n        d1 = self.conv(d1)\n        \n        u2 = self.upsampling2(d1)\n        d2 = torch.add(p3, u2)\n        \n        d3 = self.upsampling8(d2)\n        \n        out = self.fc_layer(d3)\n        out = out.view(out.shape[0], self.W, self.H)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:10.378056Z","iopub.execute_input":"2022-02-10T13:20:10.378852Z","iopub.status.idle":"2022-02-10T13:20:10.390276Z","shell.execute_reply.started":"2022-02-10T13:20:10.378687Z","shell.execute_reply":"2022-02-10T13:20:10.389565Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"### Loss Function","metadata":{}},{"cell_type":"code","source":"model = VGG16_FCN8(n_classes=1)\nmodel = model.to(\"cuda\")\nmodel = model.train()\n\nlr = 1e-3\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriteria = nn.MSELoss().to(\"cuda\")\n\nepochs = 200","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:10.392392Z","iopub.execute_input":"2022-02-10T13:20:10.392957Z","iopub.status.idle":"2022-02-10T13:20:28.473057Z","shell.execute_reply.started":"2022-02-10T13:20:10.392917Z","shell.execute_reply":"2022-02-10T13:20:28.472285Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    total_loss = 0\n    for data in tqdm(trainloader):\n        image, mask = data[\"image\"], data[\"mask\"]\n        image, mask = image.to(\"cuda\"), mask.to(\"cuda\")\n        \n        pred = model(image)\n        loss = criteria(pred, mask)\n        \n        total_loss += loss / 16 # batchsize\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    torch.save(model, \"vgg16-fcn8.pt\")\n    print(f\"epoch: {epoch+1:04d} | loss: {total_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:20:30.146789Z","iopub.execute_input":"2022-02-10T13:20:30.147376Z","iopub.status.idle":"2022-02-10T13:49:34.401766Z","shell.execute_reply.started":"2022-02-10T13:20:30.147341Z","shell.execute_reply":"2022-02-10T13:49:34.400752Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Testing (Test Dataset)","metadata":{}},{"cell_type":"code","source":"# Function to view video\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\ndef play(filename):\n    html = ''\n    video = open(filename,'rb').read()\n    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n    html += f'<video width=1000 controls autoplay loop><source src=\"{src}\" type=\"video/mp4\"></video>' \n    return HTML(html)\n\n# Function to calculate mask over image\ndef weighted_img(img, initial_img, α=1., β=0.5, γ=0.):\n    return cv2.addWeighted(initial_img, α, img, β, γ)\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((IMG_SIZE, IMG_SIZE))\n])\n\n# Function to process an individual image\ndef process_image(image):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    img = Image.fromarray(image)\n    \n    # Preprocess image\n    img = transform(img)\n    img = img.unsqueeze_(0)\n    img = img.to(\"cuda\")\n    \n    # Get the binary mask\n    pred_mask = model(img)\n    pred_mask = pred_mask.detach().cpu().numpy()\n    \n    pred_mask = pred_mask.transpose((1, 2, 0))\n    mask = np.round_(pred_mask)\n    \n    # Convert to mask image\n    zero_image = np.zeros_like(mask)\n    mask = np.dstack((mask, zero_image, zero_image)) * 255\n    mask = np.asarray(mask, np.uint8)\n    \n    # Get the final image|\n    image = np.asarray(image, np.uint8)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    \n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    mask = cv2.cvtColor(mask, cv2.COLOR_RGB2BGR)\n    \n    final_image = weighted_img(mask, image)\n    final_image = cv2.resize(final_image, (1280, 720))\n\n    return final_image","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:53:02.023175Z","iopub.execute_input":"2022-02-10T13:53:02.023440Z","iopub.status.idle":"2022-02-10T13:53:02.034995Z","shell.execute_reply.started":"2022-02-10T13:53:02.023412Z","shell.execute_reply":"2022-02-10T13:53:02.034059Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Make a new directory\nif not \"videos\" in os.listdir(\".\"):\n    os.mkdir(\"videos\")\n    print(\"mkdir videos\")\n\n# Creating a VideoCapture object to read the video\n# project_video = \"challenge.mp4\"\n# project_video = \"challenge_video.mp4\"\nproject_video = \"harder_challenge_video.mp4\"\n\n\noriginal_video = cv2.VideoCapture(test_data_dir + project_video)\nframe_width = int(original_video.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(original_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = original_video.get(cv2.CAP_PROP_FPS)\n\nprint(f\"Wori: {frame_width}, Hori: {frame_height}, fps: {fps}\")\n\n# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\nfourcc = cv2.VideoWriter_fourcc('m','p','4','v')\noutput = cv2.VideoWriter(\"./videos/\"+project_video, fourcc, fps, (frame_width,frame_height))\n\n# Process Video\nwhile(original_video.isOpened()):\n    ret, frame = original_video.read()\n\n    if ret == True:\n        # Write the frame into the file 'output.avi'\n        output.write(process_image(frame))\n    else:\n        break\n\n# When everything done, release the video capture and video write objects\noriginal_video.release()\noutput.release()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:58:10.128483Z","iopub.execute_input":"2022-02-10T13:58:10.128757Z","iopub.status.idle":"2022-02-10T13:58:47.923244Z","shell.execute_reply.started":"2022-02-10T13:58:10.128727Z","shell.execute_reply":"2022-02-10T13:58:47.922495Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# play(\"../input/kittiroadsegmentation/testing/challenge_video.mp4\")\n!ls\nplay(\"./videos/challenge.mp4\")","metadata":{"execution":{"iopub.status.busy":"2022-02-10T13:53:10.448323Z","iopub.execute_input":"2022-02-10T13:53:10.448581Z","iopub.status.idle":"2022-02-10T13:53:11.544405Z","shell.execute_reply.started":"2022-02-10T13:53:10.448547Z","shell.execute_reply":"2022-02-10T13:53:11.541806Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"play(\"videos/\" + project_video)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T17:26:18.24838Z","iopub.status.idle":"2021-11-02T17:26:18.248994Z","shell.execute_reply.started":"2021-11-02T17:26:18.248766Z","shell.execute_reply":"2021-11-02T17:26:18.24879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\n\n- [Kitti Dataset Processing](http://ronny.rest/blog/post_2017_09_06_kitti_road_data/)\n- [Image Segmentation on Keras](https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation/)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}